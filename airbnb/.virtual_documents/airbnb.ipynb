import pandas as pd
# import polars as pl
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns





listing_df = pd.read_csv("data/Listings.csv", sep=",", encoding="latin1", low_memory=False)


listing_df.info()


listing_df.shape


listing_df.sample(n=5)


listing_df.isna().sum()


listing_df[["neighbourhood", "minimum_nights", "property_type", "accommodates", "bedrooms", "price"]].sample(10)


listing_df.nunique()


listing_df.describe()


listing_df['host_since'] = listing_df['host_since'].apply(pd.to_datetime, errors='coerce')


df = listing_df.copy()


# clip extreme max_nights (Airbnb often caps at 365 or 1000 in practice)
df['maximum_nights'] = df['maximum_nights'].clip(upper=365*2) 


df = df.drop_duplicates(subset=['listing_id'])


print(df.isna().sum().sort_values(ascending=False).head(10))


# Handling Outlier in price, 


def remove_outliers_iqr(df, column, threshold=1.5):
    q1 = df[column].quantile(0.25)
    q3 = df[column].quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - (iqr * threshold)
    upper_bound = q3 + (iqr * threshold)
    df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]
    return df


# Remove outliers from the 'total_sales' column
df = remove_outliers_iqr(df, 'price')
df = remove_outliers_iqr(df, 'minimum_nights')
df = remove_outliers_iqr(df, 'maximum_nights')


df.describe()





# ---- 1. Price Distribution ----
plt.figure(figsize=(8,5))
sns.histplot(df['price'], bins=100, kde=True)
plt.xlim(0, 1000)   # focus on most listings (avoid outliers 600k+)
plt.title("Distribution of Listing Prices (Capped at $1000)")
plt.xlabel("Price ($)")
plt.ylabel("Count")
plt.show()

# ---- 2. Bedrooms Distribution ----
plt.figure(figsize=(8,5))
sns.countplot(x='bedrooms', data=df, order=df['bedrooms'].value_counts().index[:15])
plt.title("Top 15 Bedroom Counts in Listings")
plt.xlabel("Bedrooms")
plt.ylabel("Count")
plt.show()

# ---- 3. Accommodates Distribution ----
plt.figure(figsize=(8,5))
sns.countplot(x='accommodates', data=df, order=df['accommodates'].value_counts().index)
plt.title("Guests Accommodated per Listing")
plt.xlabel("Number of Guests")
plt.ylabel("Count")
plt.show()

# ---- 4. Review Scores ----
plt.figure(figsize=(8,5))
sns.histplot(df['review_scores_rating'].dropna(), bins=30, kde=True)
plt.title("Distribution of Review Scores")
plt.xlabel("Review Rating")
plt.ylabel("Count")
plt.show()

# ---- 5. Price by Room Type ----
plt.figure(figsize=(8,5))
sns.boxplot(x='room_type', y='price', data=df)
plt.ylim(0, 1000)
plt.title("Price by Room Type (Capped at $1000)")
plt.show()





# Can you spot any major differences in the Airbnb market between cities?


df['host_is_superhost'] = df['host_is_superhost'].map({'t': 1, 'f': 0}).fillna(0)

city_superhost_df = df[["city", "host_is_superhost"]].groupby(["city"]).agg({
    "host_is_superhost":"sum"
}).reset_index()


city_superhost_df.sort_values(by="host_is_superhost", ascending=False)


N = 10  # Change this to show more or fewer cities
plt.figure(figsize=(12,6))
sns.barplot(data=city_superhost_df.head(N), x="city", y="host_is_superhost")
plt.title("Number of Superhosts by City")
plt.xlabel("City")
plt.ylabel("Number of Superhosts")
plt.xticks(rotation=90)  # Rotate x-axis labels for better readability
plt.tight_layout()  # Ensure labels fit within the plot area
plt.show()


city_roomtype_df = df.groupby(["city", "room_type"]).size().reset_index(name="count")

plt.figure(figsize=(12,6))
sns.barplot(data=city_roomtype_df, x="city", y="count", hue="room_type")
plt.title("Room Type Distribution by City")
plt.xlabel("City")
plt.ylabel("Number of Listings")
plt.xticks(rotation=90)
plt.legend(title="Room Type")
plt.tight_layout()
plt.show()


# Total bedrooms per city
city_bedrooms_df = df.groupby("city")["bedrooms"].sum().reset_index()

plt.figure(figsize=(12,6))
sns.barplot(data=city_bedrooms_df.sort_values("bedrooms", ascending=False).head(10),
            x="city", y="bedrooms")
plt.title("Total Bedrooms by City")
plt.xlabel("City")
plt.ylabel("Total Bedrooms")
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()



# Total accomodation per city
city_accommodates_df = df.groupby("city")["accommodates"].sum().reset_index()

plt.figure(figsize=(12,6))
sns.barplot(data=city_accommodates_df.sort_values("accommodates", ascending=False).head(10),
            x="city", y="accommodates")
plt.title("Total Guest Capacity by City")
plt.xlabel("City")
plt.ylabel("Total Accommodates (Guests)")
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()



# Cap minimum_nights at a reasonable threshold
df["minimum_nights"] = df["minimum_nights"].clip(lower=1, upper=366)

# Cap maximum_nights at 366
df["maximum_nights"] = df["maximum_nights"].clip(lower=1, upper=366)


city_minnights_df = df.groupby("city")["minimum_nights"].mean().reset_index()

plt.figure(figsize=(12,6))
sns.barplot(data=city_minnights_df.sort_values("minimum_nights", ascending=False).head(10),
            x="city", y="minimum_nights")
plt.title("Average Minimum Nights by City")
plt.xlabel("City")
plt.ylabel("Avg. Minimum Nights")
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()


city_maxnights_df = df.groupby("city")["maximum_nights"].mean().reset_index()

plt.figure(figsize=(12,6))
sns.barplot(data=city_maxnights_df.sort_values("maximum_nights", ascending=False).head(10),
            x="city", y="maximum_nights")
plt.title("Average Maximum Nights by City")
plt.xlabel("City")
plt.ylabel("Avg. Maximum Nights")
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()






# Which attributes have the biggest influence in price?


df = df.dropna(subset=['host_is_superhost', 'host_total_listings_count'])


drop_cols_1 = ["listing_id", "name", "host_id",  "host_location", "reviewer_id", "district", "date"]
df = df.drop(columns=[c for c in drop_cols if c in df.columns])

# initially working with non non cols
drop_cols_2 = ([
    "host_response_time", "host_response_rate", "host_acceptance_rate", 
    "bedrooms", "review_scores_rating", "review_scores_accuracy", 
    "review_scores_cleanliness", "review_scores_checkin", "review_scores_communication",
    "review_scores_location", "review_scores_value", "neighbourhood"
])
df = df.drop(columns=[c for c in drop_cols_2 if c in df.columns])

X= df.drop(columns=["price"])
y= df["price"]


X


# Handling list of amenities
X["amenities"] = X["amenities"].astype(str)
X["amenities_count"] = X["amenities"].apply(lambda x: len(x.replace("[","").replace("]","").replace("\"","").replace("\'","").split(",")))
X = X.drop(columns=["amenities"])

# Handling Host_since "YYYY-MM-DD"
NO_OF_DAYS_IN_YEAR = 365
# YEAR_OF_AIRBNB_START = 2008
X["host_since"] = pd.to_datetime(X["host_since"], errors="coerce")
today = pd.to_datetime("today")
X["host_experience_years"] = (today - X["host_since"]).dt.days // NO_OF_DAYS_IN_YEAR
X = X.drop(columns=["host_since"])


# X['host_is_superhost'] = X['host_is_superhost'].map({'t': 1, 'f': 0}).fillna(0)
X['host_has_profile_pic'] = X['host_has_profile_pic'].map({'t': 1, 'f': 0}).fillna(0)
X['host_identity_verified'] = X['host_identity_verified'].map({'t': 1, 'f': 0}).fillna(0)
X['instant_bookable'] = X['instant_bookable'].map({'t': 1, 'f': 0}).fillna(0)





from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.cluster import KMeans
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, cross_val_score


# Feature groups
# "bedrooms" was removed from numerical, i will add it back if needed
numeric_features = ["accommodates", "minimum_nights", "maximum_nights",
                    "host_total_listings_count", "amenities_count", "host_experience_years"]
categorical_features = ["property_type", "room_type", "city", "instant_bookable",
                        "host_is_superhost", "host_has_profile_pic", "host_identity_verified"]

geo_features = ["latitude", "longitude"]  # for clustering



# Preprocessors

# numeric → scaling
numeric_transformer = Pipeline(steps=[
    ("scaler", StandardScaler())
])

# categorical → one hot
categorical_transformer = Pipeline(steps=[
    ("encoder", OneHotEncoder(handle_unknown="ignore"))
])

# geo → cluster + drop lat/lon
geo_transformer = Pipeline(steps=[
    ("cluster", KMeans(n_clusters=20, random_state=42))  # will tune later
])


# Full preprocessing
preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features),
        ("geo", geo_transformer, geo_features)
    ],
    remainder="drop"
)


# Flexible Model Pipeline
def make_pipeline(model):
    return Pipeline(steps=[
        ("preprocessor", preprocessor),
        ("regressor", model)
    ])


models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42)
}


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

for name, model in models.items():
    pipeline = make_pipeline(model)
    scores_mae = cross_val_score(pipeline, X_train, y_train, cv=5, scoring="neg_mean_absolute_error")
    # scores_r2 = cross_val_score(pipeline, X_train, y_train, cv=5, scoring="r2")
    print(f"{name} - Mean MAE: {-scores_mae.mean():.2f}, Std: {scores_mae.std():.2f}")
    # print(f"{name} - Mean R2: {scores_r2.mean():.2f}, Std: {scores_r2.std():.2f}")





expected = set(numeric_features + categorical_features + geo_features)
missing_in_X = sorted(list(expected - set(X.columns)))
extra_in_X = sorted(list(set(X.columns) - expected - {"amenities_count", "host_expericence_years"}))

na_counts = X[list(expected & set(X.columns))].isna().sum().sort_values(ascending=False)
y_na = y.isna().sum()

cat_dtypes = X[categorical_features].dtypes.astype(str).to_dict()
cardinality = {c: int(X[c].nunique()) for c in categorical_features if c in X.columns}

print("Missing expected features in X:", missing_in_X)
print("Unexpected extra features in X:", extra_in_X)
print("Top NA counts (features):")
print(na_counts[na_counts > 0].head(10))
print("y NA count:", int(y_na))
print("Categorical dtypes:", cat_dtypes)
print("Categorical cardinality:", cardinality)


from sklearn.metrics import mean_absolute_error, r2_score

results = []
for name, model in models.items():
    pipe = make_pipeline(model)
    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    preproc = pipe.named_steps["preprocessor"]
    Xt_train = preproc.transform(X_train)
    n_features = Xt_train.shape[1] if hasattr(Xt_train, "shape") else None
    results.append((name, mae, r2, n_features))

baseline = float(y_train.median())
baseline_mae = mean_absolute_error(y_test, np.full_like(y_test, baseline, dtype=float))

print("Baseline MAE (train median):", round(baseline_mae, 2))
for name, mae, r2, n_features in results:
    print(f"{name}: MAE={mae:.2f}, R2={r2:.3f}, n_features_after_preproc={n_features}")





# Clustering listings using their Logitude and Latitude
from sklearn.metrics import silhouette_score

geo = X[["latitude", "longitude"]]
# Determine the optimal number of clusters using the Elbow Method
sse = []
silhouette = []
K_range = range(2, 21)

for k in K_range:
    k_means = KMeans(n_clusters=k, random_state=42)
    k_means.fit(geo)
    sse.append(k_means.inertia_)
    silhouette.append(silhouette_score(geo, k_means.labels_))

# Plotting the Elbow Method
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(K_range, sse, marker='o')#"'bx-'
plt.xlabel('Number of clusters (k)')
plt.ylabel('Sum of squared distances (SSE)')
plt.title('Elbow Method for Optimal k')


# Are you able to identify any trends or seasonality in the review data?





# Which city offers a better value for travel?



