import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns





listing_df = pd.read_csv("data/Listings.csv", sep=",", encoding="latin1", low_memory=False)


listing_df.info()


listing_df.shape


listing_df.sample(n=5)


listing_df.isna().sum()


listing_df[["neighbourhood", "minimum_nights", "property_type", "accommodates", "bedrooms", "price"]].sample(10)


listing_df.nunique()


listing_df.describe()





listing_df['host_since'] = listing_df['host_since'].apply(pd.to_datetime, errors='coerce')


df = listing_df.copy()


# clip extreme max_nights (Airbnb often caps at 365 or 1000 in practice)
df["minimum_nights"] = df["minimum_nights"].clip(lower=1, upper=366)
df["maximum_nights"] = df["maximum_nights"].clip(lower=1, upper=366)


df = df.drop_duplicates(subset=['listing_id'])


print(df.isna().sum().sort_values(ascending=False).head(10))


# Handling Outlier in price,


def remove_outliers_iqr(dataframe, column, threshold=1.5):
    q1 = dataframe[column].quantile(0.25)
    q3 = dataframe[column].quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - (iqr * threshold)
    upper_bound = q3 + (iqr * threshold)
    dataframe = dataframe[(dataframe[column] >= lower_bound) & (dataframe[column] <= upper_bound)]
    return dataframe


# Remove outliers from the 'total_sales' column
df = remove_outliers_iqr(df, 'price')
df = remove_outliers_iqr(df, 'minimum_nights')
df = remove_outliers_iqr(df, 'maximum_nights')


df.describe()





df.isna().sum().sort_values(ascending=False)





# ---- 1. Price Distribution ----
plt.figure(figsize=(8,5))
sns.histplot(df['price'], bins=100, kde=True)
plt.xlim(0, 1000)   # focus on most listings (avoid outliers 600k+)
plt.title("Distribution of Listing Prices (Capped at $1000)")
plt.xlabel("Price ($)")
plt.ylabel("Count")
plt.show()

# ---- 2. Bedrooms Distribution ----
plt.figure(figsize=(8,5))
sns.countplot(x='bedrooms', data=df, order=df['bedrooms'].value_counts().index[:15])
plt.title("Top 15 Bedroom Counts in Listings")
plt.xlabel("Bedrooms")
plt.ylabel("Count")
plt.show()

# ---- 3. Accommodates Distribution ----
plt.figure(figsize=(8,5))
sns.countplot(x='accommodates', data=df, order=df['accommodates'].value_counts().index)
plt.title("Guests Accommodated per Listing")
plt.xlabel("Number of Guests")
plt.ylabel("Count")
plt.show()

# ---- 4. Review Scores ----
plt.figure(figsize=(8,5))
sns.histplot(df['review_scores_rating'].dropna(), bins=30, kde=True)
plt.title("Distribution of Review Scores")
plt.xlabel("Review Rating")
plt.ylabel("Count")
plt.show()

# ---- 5. Price by Room Type ----
plt.figure(figsize=(8,5))
sns.boxplot(x='room_type', y='price', data=df)
plt.ylim(0, 1000)
plt.title("Price by Room Type (Capped at $1000)")
plt.show()





# Can you spot any major differences in the Airbnb market between cities?


df['host_is_superhost'] = df['host_is_superhost'].map({'t': 1, 'f': 0}).fillna(0)

city_superhost_df = df[["city", "host_is_superhost"]].groupby(["city"]).agg({
    "host_is_superhost":"sum"
}).reset_index()


city_superhost_df.sort_values(by="host_is_superhost", ascending=False)


no_of_city = 10
plt.figure(figsize=(12,6))
sns.barplot(data=city_superhost_df.head(no_of_city), x="city", y="host_is_superhost")
plt.title("Number of Superhosts by City")
plt.xlabel("City")
plt.ylabel("Number of Superhosts")
plt.xticks(rotation=90)  # Rotate x-axis labels for better readability
plt.tight_layout()  # Ensure labels fit within the plot area
plt.show()


city_roomtype_df = df.groupby(["city", "room_type"]).size().reset_index(name="count")

plt.figure(figsize=(12,6))
sns.barplot(data=city_roomtype_df, x="city", y="count", hue="room_type")
plt.title("Room Type Distribution by City")
plt.xlabel("City")
plt.ylabel("Number of Listings")
plt.xticks(rotation=90)
plt.legend(title="Room Type")
plt.tight_layout()
plt.show()


# Total bedrooms per city
city_bedrooms_df = df.groupby("city")["bedrooms"].sum().reset_index()

plt.figure(figsize=(12,6))
sns.barplot(data=city_bedrooms_df.sort_values("bedrooms", ascending=False).head(10),
            x="city", y="bedrooms")
plt.title("Total Bedrooms by City")
plt.xlabel("City")
plt.ylabel("Total Bedrooms")
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()



# Total accomodation per city
city_accommodates_df = df.groupby("city")["accommodates"].sum().reset_index()

plt.figure(figsize=(12,6))
sns.barplot(data=city_accommodates_df.sort_values("accommodates", ascending=False).head(10),
            x="city", y="accommodates")
plt.title("Total Guest Capacity by City")
plt.xlabel("City")
plt.ylabel("Total Accommodates (Guests)")
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()



# Box-and-whisker plots of minimum and maximum nights by top 10 cities (by average)

# Top 10 cities by average minimum_nights
top_min_cities = (
    df.groupby("city")["minimum_nights"].mean().sort_values(ascending=False).head(10).index
)
min_data = df[df["city"].isin(top_min_cities)]

plt.figure(figsize=(12, 6))
sns.boxplot(data=min_data, x="city", y="minimum_nights", order=top_min_cities)
plt.title("Minimum Nights by City (Box and Whiskers)")
plt.xlabel("City")
plt.ylabel("Minimum Nights")
plt.ylim(0, 10)
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

# Top 10 cities by average maximum_nights
top_max_cities = (
    df.groupby("city")["maximum_nights"].mean().sort_values(ascending=False).head(10).index
)
max_data = df[df["city"].isin(top_max_cities)]

plt.figure(figsize=(12, 6))
sns.boxplot(data=max_data, x="city", y="maximum_nights", order=top_max_cities)
plt.title("Maximum Nights by City (Box and Whiskers)")
plt.xlabel("City")
plt.ylabel("Maximum Nights")
plt.ylim(0, 500)
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()






# Which attributes have the biggest influence on price?


df = df.dropna(subset=['host_is_superhost', 'host_total_listings_count'])


drop_cols = ([
    "listing_id", "name", "host_id",  "host_location", "reviewer_id", "district", "date",#Highly unique columns
    "host_response_time", "host_response_rate", "host_acceptance_rate", #High level of NaN value (to bottom)
    "bedrooms", "review_scores_rating", "review_scores_accuracy", 
    "review_scores_cleanliness", "review_scores_checkin", "review_scores_communication",
    "review_scores_location", "review_scores_value", "neighbourhood"
])
df = df.drop(columns=[c for c in drop_cols if c in df.columns])

X= df.drop(columns=["price"])
y= df["price"]


# Handling list of amenities
X["amenities"] = X["amenities"].astype(str)
X["amenities_count"] = X["amenities"].apply(lambda x: len(x.replace("[","").replace("]","").replace("\"","").replace("\'","").split(",")))
X = X.drop(columns=["amenities"])

# Handling Host_since "YYYY-MM-DD"
NO_OF_DAYS_IN_YEAR = 365
# YEAR_OF_AIRBNB_START = 2008
X["host_since"] = pd.to_datetime(X["host_since"], errors="coerce")
today = pd.to_datetime("today")
X["host_experience_years"] = (today - X["host_since"]).dt.days // NO_OF_DAYS_IN_YEAR
X = X.drop(columns=["host_since"])


X


# X['host_is_superhost'] = X['host_is_superhost'].map({'t': 1, 'f': 0}).fillna(0)
X['host_has_profile_pic'] = X['host_has_profile_pic'].map({'t': 1, 'f': 0}).fillna(0)
X['host_identity_verified'] = X['host_identity_verified'].map({'t': 1, 'f': 0}).fillna(0)
X['instant_bookable'] = X['instant_bookable'].map({'t': 1, 'f': 0}).fillna(0)





from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.cluster import KMeans
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, cross_val_score


# Feature groups
# "bedrooms" was removed from numerical, i will add it back if needed
numeric_features = ["accommodates", "minimum_nights", "maximum_nights",
                    "host_total_listings_count", "amenities_count", "host_experience_years"]
categorical_features = ["property_type", "room_type", "city", "instant_bookable",
                        "host_is_superhost", "host_has_profile_pic", "host_identity_verified"]

geo_features = ["latitude", "longitude"]  # for clustering



# Picking optimum number of clusters
from sklearn.metrics import silhouette_score

# geo = X[["latitude", "longitude"]]
geo = X[["latitude", "longitude"]].sample(5000)#to save time i choose a small sample
# Determine the optimal number of clusters using the Elbow Method
sse = []
silhouette = []
K_range = range(2, 21)

for k in K_range:
    k_means = KMeans(n_clusters=k, random_state=42)
    k_means.fit(geo)
    sse.append(k_means.inertia_)
    silhouette.append(silhouette_score(geo, k_means.labels_))

# Plotting the Elbow Method
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(K_range, sse, marker='o')#"'bx-'
plt.xlabel('Number of clusters (k)')
plt.ylabel('Sum of squared distances (SSE)')
plt.title('Elbow Method for Optimal k')





# PLot a cluster of 8 log and lat dynamically
def plot_cluster_map(X, y=None, kmeans=None, title="8-Cluster Map (Latitude/Longitude)"):
    # Ensure plotly is available (install once if missing)
    try:
        import plotly.express as px
        import plotly.graph_objects as go
    except ImportError:
        !pip install plotly
        import plotly.express as px
        import plotly.graph_objects as go

    # Validate input
    required_cols = ["latitude", "longitude"]
    for col in required_cols:
        if col not in X.columns:
            raise ValueError(f"Missing required column: {col}")

    geo_df = X[required_cols].dropna().copy()
    if geo_df.empty:
        raise ValueError("No valid latitude/longitude rows to plot.")

    # Fit or validate a KMeans with 8 clusters on lat/lon
    if (kmeans is None) or (getattr(kmeans, "n_clusters", None) != 8):
        kmeans = KMeans(n_clusters=8, random_state=42)
        kmeans.fit(geo_df)
    elif not hasattr(kmeans, "cluster_centers_"):
        kmeans.fit(geo_df)

    labels = kmeans.predict(geo_df)
    geo_df["cluster"] = labels.astype(int)

    # Optional hover fields if present
    hover_fields = [c for c in ["city", "price", "property_type", "room_type"] if c in X.columns]
    plot_df = geo_df.join(X[hover_fields], how="left")

    # Sample to avoid overplotting
    max_points = 10000
    if len(plot_df) > max_points:
        plot_df = plot_df.sample(n=max_points, random_state=42)

    # Compute map center and prepare centroids
    center_lat = float(plot_df["latitude"].mean())
    center_lon = float(plot_df["longitude"].mean())
    centers = kmeans.cluster_centers_
    centers_lat = centers[:, 0]
    centers_lon = centers[:, 1]

    # Build interactive map
    fig = px.scatter_map(
        plot_df,
        lat="latitude",
        lon="longitude",
        color="cluster",
        hover_data=hover_fields,
        title=title,
        height=650,
        zoom=1
    )
    fig.update_layout(
        mapbox_style="open-street-map",
        mapbox_center={"lat": center_lat, "lon": center_lon},
        legend_title_text="Cluster",
        margin=dict(l=10, r=10, t=50, b=10),
        hovermode="closest"
    )

    # Add cluster centers
    fig.add_trace(go.Scattermapbox(
        lat=centers_lat,
        lon=centers_lon,
        mode="markers+text",
        marker=go.scattermapbox.Marker(size=14, color="black", opacity=0.8),
        text=[f"C{i}" for i in range(8)],
        textposition="top center",
        name="Centroids"
    ))

    fig.show()



plot_cluster_map(X=X)


# Preprocessors

# numeric → scaling
numeric_transformer = Pipeline(steps=[
    ("scaler", StandardScaler())
])

# categorical → one hot
categorical_transformer = Pipeline(steps=[
    ("encoder", OneHotEncoder(handle_unknown="ignore"))
])

# geo → cluster + drop lat/lon
geo_transformer = Pipeline(steps=[
    ("cluster", KMeans(n_clusters=8, random_state=42)),  # Choose 8 based on the above graph
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])


# Full preprocessing
preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features),
        ("geo", geo_transformer, geo_features)
    ],
    remainder="drop"
)


# Flexible Model Pipeline
def make_pipeline(regressor_model):
    return Pipeline(steps=[
        ("preprocessor", preprocessor),
        ("regressor", regressor_model)
    ])





from sklearn.linear_model import Lasso

# Lasso is lightweight and suitable for resource-constrained settings:
# - Adds L1 regularization → sparsity after OHE, smaller/faster model
# - Handles multicollinearity among correlated features
# - Fast training and inference compared to complex ensembles
models = {
    "Linear Regression": LinearRegression(),
    "Lasso Regression": Lasso(alpha=0.001, max_iter=10000, tol=1e-3),
    # "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42)
}



# Example
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

for name, model in models.items():
    pipeline = make_pipeline(model)
    scores_mae = cross_val_score(pipeline, X_train, y_train, cv=5, scoring="neg_mean_absolute_error")
    scores_r2 = cross_val_score(pipeline, X_train, y_train, cv=5, scoring="r2")
    print(f"{name} - Mean MAE: {-scores_mae.mean():.2f}, Std: {scores_mae.std():.2f}")
    print(f"{name} - Mean R2: {scores_r2.mean():.2f}, Std: {scores_r2.std():.2f}")


# Permutation Importance for Linear Regression
from sklearn.inspection import permutation_importance

pipe = make_pipeline(models["Linear Regression"])
pipe.fit(X_train, y_train)

# Compute permutation importance on the test set
result = permutation_importance(pipe, X_test, y_test, n_repeats=10, random_state=42)

# Get feature names after preprocessing
feature_names = (
    numeric_features +
    list(pipe
         .named_steps["preprocessor"].transformers_[1][1]
         .named_steps["encoder"].get_feature_names_out(categorical_features))
    + ["geo_cluster"]
)

importance = pd.DataFrame({
    "Feature": feature_names,
    "Importance": result.importances_mean,
    "Std": result.importances_std
}).sort_values(by="Importance", ascending=False)

print(importance.head(10))


# Permutation Importance for Lasso Regression
from sklearn.inspection import permutation_importance

pipe = make_pipeline(models["Lasso Regression"])
pipe.fit(X_train, y_train)

# Compute permutation importance on the test set
result = permutation_importance(pipe, X_test, y_test, n_repeats=10, random_state=42)

# Get feature names after preprocessing
feature_names = (
    numeric_features +
    list(pipe
         .named_steps["preprocessor"].transformers_[1][1]
         .named_steps["encoder"].get_feature_names_out(categorical_features))
    + ["geo_cluster"]
)

importance = pd.DataFrame({
    "Feature": feature_names,
    "Importance": result.importances_mean,
    "Std": result.importances_std
}).sort_values(by="Importance", ascending=False)

print(importance.head(10))





# Are you able to identify any trends or seasonality in the review data?


reviews_df = pd.read_csv("data/Reviews.csv", sep=",", encoding="latin1", low_memory=False)
reviews_df["date"] = pd.to_datetime(reviews_df["date"], errors="coerce")
reviews_df.head()


reviews_df.info()


reviews_df.isna().sum()


# Monthly review volume and simple seasonality helpers
monthly_reviews = reviews_df.groupby(pd.Grouper(key="date", freq="MS")).size()


print(monthly_reviews.info())
print(monthly_reviews.head())


# Ensure index is datetime and freq is set
monthly_reviews = monthly_reviews.asfreq("MS")
# Fill missing months with 0 (or interpolate if more realistic)
monthly_reviews = monthly_reviews.fillna(0)
# Convert to float (required by ARIMA/ETS)
monthly_reviews = monthly_reviews.astype(float)


# plotting the whole time series with an interactive plot
# !pip install plotly

import plotly.express as px

# Prepare data: monthly_reviews is a Series with a DatetimeIndex
ts = monthly_reviews
if isinstance(ts, pd.Series):
    ts = ts.reset_index(name="reviews")  # 'date' index becomes a column

fig = px.line(ts, x="date", y="reviews", title="Monthly Reviews Over Time (Interactive)")
fig.update_layout(
    xaxis_title="Date",
    yaxis_title="Number of Reviews",
    hovermode="x unified",
)
fig.update_xaxes(rangeslider_visible=True)
fig.show()



ts.info()


monthly_reviews.info()


ts


monthly_reviews


# Decompose the time series using statsmodels
from statsmodels.tsa.seasonal import STL

# monthly_reviews = monthly_reviews.asfreq("MS").fillna(0)  # Ensure regular frequency and fill missing months
stl = STL(monthly_reviews, period=12)
result = stl.fit()

fig = result.plot()
fig.set_size_inches(12, 8)
plt.suptitle("STL Decomposition of Monthly Reviews", fontsize=16)
plt.show()

# showing Original, Trend, Seasonal, Residuals





# Setup
# !pip install pmdarima
# !pip install prophet
del auto_arima
from pmdarima import auto_arima
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from prophet import Prophet


# # Train/test split by time
split_date = "2018-12-31"
train = monthly_reviews.loc[:split_date]
test = monthly_reviews.loc[split_date:]
print(f"Train size: {train.shape}, Test size: {test.shape}")


#Arima
arima_model = auto_arima(train, seasonal=True, trace=True, error_action="ignore", suppress_warnings=True)

arima_forecast = pd.Series(
    arima_model.predict(n_periods=len(test)),
    index=test.index
)


# Holt-Winters Exponential Smoothing
ets_model = ExponentialSmoothing(train, trend="add", seasonal="add", seasonal_periods=12, damped_trend=True)
ets_fit = ets_model.fit()
ets_forecast = ets_fit.forecast(steps=len(test))


#Prophet
prophet_df = train.reset_index()
prophet_df.columns=["ds", "y"]

prophet_model = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)
prophet_model.fit(prophet_df)

future = prophet_model.make_future_dataframe(periods=len(test), freq="MS")
forecast = prophet_model.predict(future)

prophet_forecast = forecast.set_index("ds")["yhat"][-len(test):]


# Plot Results
plt.figure(figsize=(20,8))
plt.plot(train.index, train, label="Train")
plt.plot(test.index, test, label="Test", color="black")

plt.plot(arima_forecast.index, arima_forecast, label="ARIMA Forecast", linestyle="--")
plt.plot(ets_forecast.index, ets_forecast, label="ETS Forecast", linestyle="--")
plt.plot(prophet_forecast.index, prophet_forecast, label="Prophet Forecast", linestyle="--")

plt.title("Forecasting Airbnb Monthly Reviews (Comparing Models)")
plt.xlabel("Date")
plt.ylabel("Number of Reviews")
plt.legend()
plt.show()





from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error


def evaluate_models(train, test):
    results = {}

    # ---------------- ARIMA ----------------
    arima_model = auto_arima(train, seasonal=True, m=12,
                             error_action="ignore", suppress_warnings=True)
    arima_forecast = pd.Series(arima_model.predict(n_periods=len(test)), index=test.index)
    results['ARIMA'] = {
        "RMSE": np.sqrt(mean_squared_error(test, arima_forecast)),
        "MAPE": mean_absolute_percentage_error(test, arima_forecast)
    }

    # ---------------- ETS ----------------
    ets_model = ExponentialSmoothing(train, trend="add", seasonal="add", seasonal_periods=12)
    ets_fit = ets_model.fit()
    ets_forecast = ets_fit.forecast(len(test))
    results['ETS'] = {
        "RMSE": np.sqrt(mean_squared_error(test, ets_forecast)),
        "MAPE": mean_absolute_percentage_error(test, ets_forecast)
    }

    # ---------------- Prophet ----------------
    prophet_df = train.reset_index()
    prophet_df.columns = ["ds", "y"]
    prophet_model = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)
    prophet_model.fit(prophet_df)

    future = prophet_model.make_future_dataframe(periods=len(test), freq="MS")
    forecast = prophet_model.predict(future)
    prophet_forecast = forecast.set_index("ds")['yhat'][-len(test):]

    results['Prophet'] = {
        "RMSE": np.sqrt(mean_squared_error(test, prophet_forecast)),
        "MAPE": mean_absolute_percentage_error(test, prophet_forecast)
    }

    return pd.DataFrame(results).T

results_df = evaluate_models(train, test)
results_df


#
plt.figure(figsize=(15, 8))
plt.plot(test.index, test, label="Actual (Test)", color="black")

plt.plot(test.index, arima_forecast, label="ARIMA Forecast", linestyle="--")
plt.plot(test.index, ets_forecast, label="ETS Forecast", linestyle="--")
plt.plot(test.index, prophet_forecast, label="Prophet Forecast", linestyle="--")

plt.title("Forecast Comparison: ARIMA vs ETS vs Prophet")
plt.xlabel("Date")
plt.ylabel("Number of Reviews")
plt.legend()
plt.show()


# Run again but truncate anomaly in 2020
split_date = "2018-12-31"
truncate_date = "2020-12-31"

train = monthly_reviews.loc[:split_date]
covid_test = monthly_reviews.loc[split_date:truncate_date]

covid_results_df = evaluate_models(train, covid_test)
covid_results_df


plt.figure(figsize=(15, 8))
plt.plot(covid_test.index, covid_test, label="Actual (Test)", color="black")

plt.plot(covid_test.index, arima_forecast, label="ARIMA Forecast", linestyle="--")
plt.plot(covid_test.index, ets_forecast, label="ETS Forecast", linestyle="--")
plt.plot(covid_test.index, prophet_forecast, label="Prophet Forecast", linestyle="--")

plt.title("Forecast Comparison: ARIMA vs ETS vs Prophet")
plt.xlabel("Date (pre-covid)")
plt.ylabel("Number of Reviews")
plt.legend()
plt.show()


# Which city offers a better value for travel?


city_price_review_df = listing_df[["city", "price", "review_scores_value"]]


city_price_review_df.isna().sum()


# drop rows where price is 0
city_price_review_df = city_price_review_df[city_price_review_df["price"] != 0]
#assign 0 to NaN in review_scores_value
city_price_review_df["review_scores_value"] = city_price_review_df["review_scores_value"].fillna(0)


city_price_review_df.isna().sum()


city_price_review_df["value_index"] = city_price_review_df["review_scores_value"] / city_price_review_df["price"]


city_price_review_df.describe(include="all")


#Aggregate by city (mean or median value index)
city_value = city_price_review_df.groupby('city').agg(
    avg_price=('price', 'mean'),
    avg_review_value=('review_scores_value', 'mean'),
    value_index=('value_index', 'mean')
).reset_index()

# sort cities by value index
city_value = city_value.sort_values(by="value_index", ascending=False)
print(city_value.head(10))


# Visualization
plt.figure(figsize=(12,6))
sns.barplot(data=city_value, x="city", y="value_index", order=city_value["city"])
plt.title("Average Value for Money by City (Airbnb)")
plt.ylabel("Value Index (Review Score ÷ log(Price))")
plt.xlabel("City")
plt.xticks(rotation=45)
plt.show()











from sklearn.model_selection import RandomizedSearchCV

# Misc 1: Tuning a hyperparameter price predictor model
param_dist = {
    'regressor__n_estimators': [50, 100, 200],
    'regressor__max_depth': [5, 10, 20, None],
    'regressor__min_samples_split': [2, 5, 10],
    'regressor__min_samples_leaf': [1, 2, 4],
    'regressor__bootstrap': [True, False]
}

search = RandomizedSearchCV(
    make_pipeline(RandomForestRegressor(random_state=42)),
    param_distributions=param_dist,
    n_iter=20,
    cv=3,
    scoring='neg_mean_absolute_error',
    random_state=42,
    n_jobs=-1,
    verbose=1
)

search.fit(X_train, y_train)
print("Best parameters found: ", search.best_params_)
best_model = search.best_estimator_
# Evaluate the best model on the test set
test_score = best_model.score(X_test, y_test)
print(f"Test set R^2: {test_score:.2f}")


# Misc 2: SHAP (on sample only)
# !pip install shap
import shap
X_test_sample = X_test.sample(1000, random_state=42)

explainer = shap.TreeExplainer(best_model["model"])
shap_values = explainer.shap_values(best_model["preprocessor"].transform(X_test_sample))

shap.summary_plot(shap_values, features=best_model["preprocessor"].transform(X_test_sample), feature_names=best_model["preprocessor"].get_feature_names_out())



